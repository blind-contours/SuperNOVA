---
title: "Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions"
author: "David McCoy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/references.bib
vignette: >
  %\VignetteIndexEntry{Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Motivation 

The motivation behind the package SuperNOVA is to address the limitations of traditional statistical methods in environmental epidemiology studies. Analysts are often interested in understanding the joint impact of a mixed exposure, i.e. a vector of exposures, but the most important variables and variable sets remain unknown. Traditional methods make overly simplistic assumptions, such as linear and additive relationships, and the resulting statistical quantities may not be directly applicable to public policy decisions.

To overcome these limitations, SuperNOVA uses data-adaptive machine learning methods to identify the variables and variable sets that have the most explanatory power on an outcome of interest. The package builds a discrete Super Learner, which is then analyzed using ANOVA style analysis to determine the variables that contribute most to the model fit through basis functions. The target parameter, which may be a single shift, effect modification, or interaction, is then applied to the data-adaptively determined variable sets.

In this way, SuperNOVA allows analysts to explore modified treatment policies and ask causal questions, such as "If exposure to collections of PFAS chemicals increases, what is the change in cholesterol, immune function, or cancer?" By providing more nuanced and data-driven insights, SuperNOVA aims to inform public policy decisions and improve our understanding of the impact of mixed exposures on health outcomes.

### Data-Adaptive Machine Learning

The package SuperNOVA uses a data-adaptive approach to identify important variables and variable sets in mixed exposure studies. To avoid over-fitting and incorrect model assumptions, the package employs cross-validation procedures. In each fold, the training data is used to:

1. Identify the variable sets of interest using flexible machine learning algorithms.
2. Fit estimators for the relevant nuisance parameters and the final target parameter of interest using the identified variable sets.
3. Obtain estimates of the nuisance parameters and the final target parameter of interest using the held-out validation data.
4. The process is repeated in each fold to provide an estimate specific to each validation data. To optimize the balance between bias and variance, SuperNOVA uses targeted minimum loss based estimation (TMLE).
5. Estimates across the folds are then averaged and a pooled fluctuation step is performed to estimate the efficient influence funciton and derive pooled variance estimates.


## Data and Notation

### Our Data-Generating System 

Consider $n$ observed units $O_1, \ldots, O_n$, where each random variable $O =(V \subset W, A, Y)$ corresponds to a single observational unit. Let $W$ denote baseline covariates (e.g., age, sex, education level) that are not considered by the 
analyst to be effect modifiers of any exposure variable, $V$ denote a subset of baseline covariates (measured before exposure) that may be potential effect modifiers of any exposure, $A$ denote a single exposure variable or a vector of
exposure variables (mixture) of interest (e.g., mixed pesticide biomarkers, metals etc. that are measured at the same time), and $Y$ an outcome of interest (e.g.,disease status). Though it need not be the case, let $A$ be continuous-valued, i.e. $A \in \mathbb{R}$. Let $O_i \sim \mathcal{P} \in \mathcal{M}$, where $\mathcal{M}$ is the nonparametric statistical model defined as the set of continuous densities on $O$ with respect to some dominating measure. To formalize the definition of stochastic interventions and their corresponding causal effects, we introduce a nonparametric structural equation model (NPSEM), based on @pearl2000causality, to define how the system changes under posited interventions on:

\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}

We denote the observed data structure $O = (W, A, Y)$.

where the set of structural equations provide a mechanistic model by which the
observed data $O$ is assumed to have been generated. 

### Assumptions

There are several standard
assumptions embedded in the NPSEM -- specifically, a temporal ordering that
supposes that $Y$ occurs after $A$, which occurs after $W$; each variable (i.e.,
$\{W, A, Y\}$) is assumed to have been generated from its corresponding
deterministic function (i.e., $\{f_W, f_A, f_Y\}$) of the observed variables
that precede it temporally, as well as an exogenous variable, denoted by $U$;
lastly, each exogenous variable is assumed to contain all unobserved causes of
the corresponding observed variable. 

### Factorizing the Likelihood

The likelihood of the data $O$ admits a factorization, wherein, for $p_0^O$,
the density of $O$ with respect to the product measure, the density evaluated
on a particular observation $o$ may be a written
\begin{equation}
  p_0^O(o) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w)
\end{equation}
where $q_{0, Y}$ is the conditional density of $Y$ given $(A, W)$ with respect
to some dominating measure, $q_{0, A}$ is the conditional density of $A$ given
$W$ with respect to dominating measure $\mu$, and $q_{0, W}$ is the density of
$W$ with respect to dominating measure $\nu$. Further, for ease of notation, let
$Q(A, W) = \mathbb{E}[Y \mid A, W]$, $g(A \mid W) = \mathbb{P}(A \mid W)$, and $q_W$ the
marginal distribution of $W$. These components of the likelihood will be
essential in developing an understanding of the manner in which stochastic
treatment regimes perturb a system and how a corresponding causal effect may be
evaluated. 

### Data-Adaptive Target Parameter: Effect Modification


For example, in `SuperNOVA` time ordering is used to determine our causal parameters of interest. That is, the analyst passes a vector $V$ of baseline covariates $W$ to `SuperNOVA` that may be effect modifiers of any mixture variable in the vector $A$. Here, the target causal parameter is effect modification of the shift parameter because $V$ is measured before $A$ in our NPSEM. The effect modification parameter is a measure of how different the counterfactual mean of $Y$ is given a shift in $A$ stratified on $V$. 

### Data-Adaptive Target Parameter: Interaction

Conversly, if sets of variables are found to be important in the data-adaptive procedure that are only in $A$ (a set of mixture components) because these variables are (in most cases) measured together at one time (such as biomarkers) the shift parameter is instead the counterfactual mean of $Y$ given a simultaneous shift in both variables in $A$. This is a measure of interaction, rather than effect modification, using our shift parameter. In the case that an individual variable in $V$ or $A$ is found to have a marginal impact on $Y$, then a simple univariate stochastic shift is done on this variable alone. 

### Overview Stochastic Shifts

Briefly, let $A$ denote a continuous-valued exposure or set of exposures, we assume that the distribution of $A$ conditional on $W = w$ has support in the interval $(l(w), u(w))$ -- for convenience, let this support be _a.e._ That is, the minimum natural value of treatment $A$ for an individual with covariates $W = w$ is $l(w)$; similarly, the maximum is $u(w)$. Then, a simple stochastic intervention, based on a shift $\delta$, may be defined

\begin{equation}\label{eqn:shift}
  d(a, w) =
  \begin{cases}
    a - \delta & \text{if } a > l(w) + \delta \\
    a & \text{if } a \leq l(w) + \delta,
  \end{cases}
\end{equation}

where $0 \leq \delta \leq u(w)$ is an arbitrary pre-specified value that defines the degree to which the observed value $A$ is to be shifted, where possible. In `SuperNOVA` there are four different shift parameters depending on what is found in the data-adaptive ANOVA within the cross-validated fold - these are discussed below. 

## Target Parameter 

Our target parameter of interest is the stochastic intervention of a data-adaptively identified exposure variable or variables. Stochastic interventions are a relatively simple yet flexible framework for defining realistic causal effects.
In contrast to intervention regimens such as the average treatment effect (ATE), stochastic interventions can be applied to nearly any type of exposure variable (binary, ordinal, continuous). The resulting stochastic shift parameter can be interpreted in a similar fashion as coefficients in a regression model.

### Types of Shifts

This `SuperNOVA` package examines the effects attributable to:

1. shifting the observed value of an individual exposure up or down 
by some scalar $\delta$.
2. shifting the observed value of an individual exposure within 
strata of an effect modifier up or down by some scalar $\delta$.
3. shifting the observed value of a joint exposure up or down by 
some scalar $\delta$.
4. shifting the observed value of a joint exposure up or down by 
some scalar $\delta$ within strata of an effect modifier. 


The `SuperNOVA` package implements algorithms to data-adaptively identify mixture variables $A$ and effect modifiers $V$ that are associated  with an outcome and for these variable sets compute one-step  or targeted minimum loss-based (TML) estimates of the counterfactual means changes induced by a shifting function $\delta(A,W)$. 

## Data-Adative Variable Set Selection
SuperNOVA first fits an unrestricted Super Learner for $E(Y|W) = h_1(W)$, where $W$ is a set of baseline covariates that the analyst wants to control for, that is, baseline covariates with no suspected effect modification with exposures $A$. Residuals are calculated, $Y^*$, and a second Super Learner is fit $E(Y^*|A,V) = h_2(A,V)$ Here, for $h_2$ we only consider models for the conditional mean of $Y^*$ given $A,V$ that are a function of linear spline terms and their tensor products. Given a fit of such a model, we can then partition the variance using classic ANOVA decompositions by testing the null that some set of coefficient values (the $\beta_{s,i}$) are 0 (e.g., all terms that contain a particular variable, or represent two-way basis functions). We can then aggregate partial F-statistics for the basis functions up to the variable level for variable/interaction importance measures. Algorithms used in `SuperNOVA` return tensor products of arbitrary order and include `earth`, `polySpline` and `hal9001`. 

### Basis functions

Indicator variables can be used to indicate if a variable $A$ is less than or equal to a specific value $a_s$. The same can be done for $A_1, A_2$ to determine if both variable are less than or equal to a specific value. Thus, a function of our outcome is written as: 

$$\psi_{\beta} = \beta_0 + \sum_{s\subset \{1,2,...,p\}}\sum_{i=1}^{n} \beta_{s,i} \phi_{s,i},
    \text{ where } \phi_{s,i} = I(\tilde{A}_{i,s} \leq w_s), A \in \mathbb{R}^p$$ and $s$ denotes indices of subsets of the $W$ (e.g., both functions of single variables and two variables).The placement of knot-points for every potential value is not feasible in most real-world scenarios and therefore `SuperNOVA` chooses the best estimator from a class of basis algorithms. This approximates the exhaustive function.
    
### Defining Important Basis Functions

The user can pass into `SuperNOVA` the parameter `quantile_thresh` which designates the F-statistic threshold by which basis functions are kept. For example, if the user uses `quantile_thresh = 0.25` and the resulting vector is $M_1, M_1V_1, M_1M_3$ then this indicates that the basis functions that had an F-statistic above the 25th quantile included basis functions for $M_1$ alone, basis functions for $M_1$ and $V_1$ and basis functions for $M_1$ and $M_3$. 

This means that, within this fold we would train estimators for each of our estimates of interest, that is, the individual stochastic shift of $M_1$, how $V_1$ modifies the shift relationship between $M_1$ and $Y$, and the joint shift of $M1,M_3$. Now let's more formally define these paramters. 

## Target Parameter: Interaction 

Once `SuperNOVA` has identified interacting variables with the highest partial F-statistic from the basis functions we need to get statistical estimation for how these interactions affect our outcome of interest.

Our target parameter of interest is a semi-parametric definition of an interaction, defined as: \begin{equation} \label{eq1}
\begin{split}
E(Y|A_{i^*} &+ \delta_1, A_{j^*} + \delta_2, W) - \\ 
E(Y|A_{i^*} &+ \delta_1, A_{j^*}, W) - \\ 
E(Y|A_{j^*} &+ \delta_2,  A_{i^*}, W) + \\
E(Y|A_{i^*}&, A_{j^*}, W) 
\end{split}
\end{equation}

For the bivariate case A $\in \{A_{i^*}, A_{j^*}$\}

This joint causal quantity of interest is identified by: $E[Y_{g^o}] = E_W [\int_a E(Y|A=a, W)g^o(a|W)]$, $g^o(a|W)$ is similar to the propensity score estimator but here we are estimating joint densities.

Where in the bivariate case: 
$$g^o(a_1, a_2|w) = g(a_1 + \delta_1, a_2 + \delta_2|W = w)$$

Which can be estimated as: 

$$g^o(a_1, a_2|w) =  g(a_1 + \delta_1|W = w) * g(a_2 + \delta_2|W = w, A = a_1)$$. 

We can write our interaction target parameter as: 

$$E[Y_{g^o_{\delta_1,\delta_2}}] - E[Y_{g^o_{\delta_1}}] - E[Y_{g^o_{\delta_2}}] + E[Y_{g^o_{1,2}}]$$ 

Where: 

\begin{equation} \label{eq2}
\begin{split}
E[Y_{g^o_{\delta_1,\delta_2}}] &= E_W [\int_{a_{1,\delta_1} a_{2, \delta_2}} E(Y|A=a_{1,\delta_1},a_{2,\delta_2} , W)g^o_{\delta_1,\delta_2}(a|W)]\\
E[Y_{g^o_{\delta_1}}] &= E_W [\int_{a_1} E(Y|A=a_{1, \delta_1}, W)g^o_{\delta_    1}(a|W)]\\
E[Y_{g^o_{\delta_2}}] &= E_W [\int_{a_2} E(Y|A=a_{2, \delta_2}, W)g^o_{\delta_2}(a|W)]\\
E[Y_{g^o_{1,2}}] &= E_W [\int_{a_1,a_2} E(Y|A=a_1,a_2, W)g^o_{1,2}(a|W)]
\end{split}
\end{equation}

We can think of this interaction target parameter as the expected outcome given a joint shift in both exposure variables marginalized over the joint densities of both variables given both delta shifts subtracted by the expected outcome given both individual shifts marginalized over each shifted density respectively and adding back in the expected outcome under no shift marginalized over the joint density under no shift intervention. 


## Target Parameter: Effect Modification

Our effect modification follows in the same vein but instead looks at the difference in expected outcome under a shift intervention in an exposure between strata of an effect modifying variable. In the bivariate case where $V$ is a binary effect modifier this looks like: 

$$E[Y_{g^o_{\delta}} | V = 1] - E[Y_{g^o_{\delta}} | V = 0]$$

Where: 

\begin{equation} \label{eq3}
\begin{split}
E[Y_{g^o_{a_\delta}}] &= E_W [\int_{a_{\delta}} E(Y|A=a_{\delta}, W)g^o_{\delta}(a | V = 1, W)]\\

E[Y_{g^o_{a_\delta}}] &= E_W [\int_{a_{\delta}} E(Y|A=a_{\delta}, W)g^o_{\delta}(a | V = 0, W)]
\end{split}
\end{equation}

In words, this can be interpreted as the expected outcome given a shift in $A$ within strata where $V = 1$, marginalized over the conditional density of A under this stratified shift and minus the expected outcome given a shift in $A$ in strata $V = 0$ marginalized over the conditional density of A under this stratified shift.


## Target Parameter: Individual Stochastic Shift

In the case where a basis function consists of only 1 variable that is included in $A$ or $V$ and individual stochastic shift is done, which is simply: 

$$E[Y_{g^o_{\delta}}]$$
Where: 

$$E[Y_{g^o_{\delta}}] = E_W [\int_{a} E(Y|A=a_{ \delta}, W)g^o_{\delta}(a|W)]$$

Or simply, the expected outcome given a shift in $A$ by $\delta$ marginalized over the conditional distribution of $a$ given such a shift.

## Estimating the Causal Effect for each Stochastic Intervention 

With identification assumptions satisfied, the efficient influence function (EIF) with respect to
the nonparametric model $\mathcal{M}$ can be estimated for each of our parameters of interested based on work from @diaz2012population and @diaz2018stochastic. The EIF for a stochastic intervention is: 

\begin{equation}
  D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
  \overline{Q}(d(a, w), w) - \Psi(P_0),
 
\end{equation}
where the auxiliary covariate $H(a,w)$ may be expressed
\begin{equation}
  H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)}
  {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w)),
  
\end{equation}
which may be reduced to
\begin{equation}
  H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1
 
\end{equation}
in the case that the treatment is within the limits that arise from conditioning
on $W$, i.e., for $A_i \in (u(w) - \delta, u(w))$.

This efficient influence function was originally derived to allow for the construction of a
semiparametric-efficient estimators for a stochastic shift in the univariate setting. The argeted maximum likelihood (TML) estimator to solve this EIF was formulated in @diaz2018stochastic by using the following steps: 

1. Construct initial density estimators $g_n$ of $g_0(A, W)$ and $Q_n$ of
   $\overline{Q}_0(A, W)$, using data-adaptive regression techniques.
2. Predict the density distribution of $d(A,W)$ under the shift $\delta$
3. For each observation $i$, compute an estimate $H_n(a_i, w_i)$ of the
   auxiliary covariate $H(a_i,w_i)$. This is the ratio of conditional densities, the shifted density in the numerator and natural density in the denominator.
4. Estimate the parameter $\epsilon$ in the logistic regression model
   $$ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
   \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),$$
   or an alternative regression model incorporating weights.
5. Compute TML estimator $\Psi_n$ of the target parameter, defining update
   $\overline{Q}_n^{\star}$ of the initial estimate
   $\overline{Q}_{n, \epsilon_n}$:
   \begin{equation}
     \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
     \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
   \end{equation}
   
### EIF and TMLE Applied to our Target Parameters


The same EIF and TMLE steps to solve the EIF used in the univariate stochastic shift of a treatment can be used for all our target parameters of interest. That is, in the case that only one variable in $A,V$ was found data-adaptively in basis functions, then the targeting step is exactly the same as in the original stochastic intervention work. In the case of effect modification, the same procedure is done however the conditional densities used in construction of the clever covariate include the counterfactuals made on the effect modifier. Construction of initial density estimators for $g_n$ of $g_0(A, V=v, W)$ and $Q_n$ of $\overline{Q}_0(A, V=v, W)$, using data-adaptive regression techniques. As can be seen, once these densities are estimated, the same TMLE steps can be done to update the original estimates. Likewise, for our interaction parameter the same procedure holds but now for joint densitieis. We construct initial density estimators $g_n$ of $g_0(A_1, A_2, W)$ and $Q_n$ of
$\overline{Q}_0(A_1, A_2, W)$, using data-adaptive regression techniques. Then our clever covariate becomes a ratio of the predicted joint likelihood of a joint shift over the unperturbed joint likelihood. The subsequent steps then all remain the same. 
  
## Data-Adaptive Esimation

The above theory works for stochastic shift situations where the $A$ is known \textit{a priori}. This is not the case for mixtures where we are interested in first identifying which variables are "important" given the fit of the best estimator and estimating the stochastic shift for these variable sets. Because the data is being used to both identify a target parameter and make estimates given this parameter, sample splitting (cross-validation) is used to avoid bias from over fitting, but still uses the entire data set to estimate a data-adaptive parameter. V-fold cross-validation involves: (i) ${1,..., n}$, observations, is divided into $V$ equal size subgroups, (ii) for each $v$, an estimation-sample, notationally $P_{n,v}$ , is defined by the v-th subgroup of size n/V, while the parameter-generating sample, $P_{n,v^c}$, is its complement. More concretely, for split $v$ the empirical distribution, $P_{n,v^c}$, is used to define the exposure(s) $A$ given our $\textit{a priori}$ basis function algorithm which outputs the target variables based on our ANOVA test of the basis functions used. The observations not in $P_{n,v^c}$, namely the empirical distribution for $P_{n,v}$ then is used to generate the parameter of interest. That is, if we had 1000 observations and 4 folds, our estimation sample $P_{n,v}$ would be of size 250. For each fold, the respective 750 observations would be used to train our estimators $g_n$ and $Q_n$ and given these estimators we then get predictions given the respective 250 observations. The predicted density, $g_n$ and outcome $Q_n$ are then used to construct our clever covariate for the shift intervention, $H_n(a_i, w_i)$ for the TMLE update within the fold. Therefore, TMLE updated estimates are given for each fold using the respective estimation sample data $P_{n,v}$. 

## Pooling Estimates Found Across the Folds

Because `SuperNOVA` gets the TMLE updated shift parameter for each condition (individual, effect modification or joint), for each fold, when many folds are used the resulting tables can be very large. For estimates that are found across all folds (are consistently found in the data adaptive procedure) we can pool the effects across the folds similar to a meta-analysis. We should therefore give effect sizes with a higher precision (i.e. a smaller standard error derived from the efficient influence function (EIF)) a greater weight. If we want to calculate the pooled effect size under the fixed-effect model, we therefore simply use a weighted average of all studies. To calculate the weight $w_k$ for each fold $k$, we can use the standard error, which we square to obtain the variance $s^2_k$ of each effect size. Since a lower variance indicates higher precision, the inverse of the variance is used to determine the weight of each fold estimate.

$$w_k = \frac{1}{s^2_k}$$

Once we know the weights, we can calculate the weighted average, our estimate of the true pooled effect $\hat{\psi}$ We only have to multiply each foldâ€™s effect size $\hat{\theta}_k$
with its corresponding weight $w_k$, sum the results across all studies $K$ in our pooled analysis, and then divide by the sum of all the individual weights.

$$\hat{\psi} = \frac{\sum_{k = 1}^K \hat{\Psi}_k w_k}{\sum_{k = 1}^K w_k}$$
Because we use the inverse of the variance, we can call this parameter the inverse-variance pooled-fold-analysis.

The variance of the combined effect across the folds is defined as the reciprocal of the sum of the weights, or $v. = \frac{1}{\sum_{i=1}^K w_i}$ and likewise the standard error is $\sqrt{v.}$. Pooled confidence intervals and p-values are calculated in the normal way based on the calculated $\hat{\psi}$ and $v.$. Below we show the results tables output by the `SuperNOVA` for each parameter for each fold then we show output from the `compute_meta_results` function which calculates the pooled esimtates and creates plots. 


## Application:

Firts, let's load the packages we'll use and set a seed for simulation:

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(kableExtra)
library(SuperNOVA)

seed <- 325911
```

Below we show implementation of `SuperNOVA` using simulated data. Because estimates are given for each data-adaptively identified parameter for each fold, after the demonstration we also show our pooled estimate approach for parameters that are found across all the folds. 

## NIEHS Mixture Workshop Data

The `SupernOVA` package comes with 2 datasets from the 2015-NIEHS-Mixtures-Workshop simulation data (https://github.com/niehs-prime/2015-NIEHS-MIxtures-Workshop). Let's load this data and run `SuperNOVA` to see if we identify 1. any interactions in the mixture variables or any marginally important mixture variables and/or 2. any effect modifying variables.

```{r NIEHS example}
data("NIEHS_data_1", package = "SuperNOVA")
```

```{r NIEH Nodes}
NIEHS_data_1$W <- rnorm(nrow(NIEHS_data_1), mean = 0, sd = 0.1)
w <- NIEHS_data_1[, c("W", "Z")]
a <- NIEHS_data_1[, c("X1", "X2", "X3", "X4", "X5", "X6", "X7")]
y <- NIEHS_data_1$Y
```


```{r run SuperNOVA NIEHS data, eval = TRUE}
deltas <- list(
  "X1" = 1, "X2" = 1, "X3" = 1,
  "X4" = 1, "X5" = 1, "X6" = 1, "X7" = 1
)

ptm <- proc.time()

NIEH_results <- SuperNOVA(
  w = w,
  a = a,
  y = y,
  deltas = deltas,
  estimator = "tmle",
  fluctuation = "standard",
  n_folds = 2,
  family = "continuous",
  quantile_thresh = 0,
  verbose = TRUE,
  parallel = TRUE,
  num_cores = 5,
  seed = seed
)

proc.time() - ptm

indiv_shift_results <- NIEH_results$`Indiv Shift Results`
em_results <- NIEH_results$`Effect Mod Results`
joint_shift_results <- NIEH_results$`Joint Shift Results`
```


Let's look at the results for the variable $X1$ which should have a positive 
effect given the data dictionary provided: 

```{r individual results}
indiv_shift_results$X1 %>%
  kableExtra::kbl(caption = "Effect Modification Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Now let's look for effect modification: 
```{r joint results}
em_results$X7Z %>%
  kableExtra::kbl(caption = "Effect Modification Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

This shows there are differential effects on the impact of shifting $X7$ between
strata of the baseline covariate Z. 

```{r interaction results}
joint_shift_results$X1X5 %>%
  kableExtra::kbl(caption = "Interaction Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

Here we see in three of the folds an interaction effect was found for $X1$ and
$X5$. 





