---
title: "Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions"
author: "David McCoy"
date: "12/5/2021"
output: html_document
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Motivation 

In many environmental epidemiology studies the analyst is interested in the joint impact of a mixed exposure. That is, rather than a single exposure $A$, $A$ is a vector of exposures of which the most important individual variables and variable sets are unknown. For example, in the analysis of a mixture of metals or air pollutants it is not known a priori what variable(s) contribute to an outcome $Y$ of interest after controlling for baseline covariates $W$ and/or what interactions/effect modifications these exposures may have. Therefore, it is necessary to identify: 

* Individual variables in a mixture that have explanatory power on an outcome
* A baseline covariate that modifies the impact of a mixture variable or set of
mixture variables
* Sets of mixture variables that have synergistic relationships.

### Data-Adaptive Machine Learning

To avoid misspecified model assumptions, it is necessary to do this automatic variable set identification procedure using data adaptive machine learning methods. In addition to using a pre-specified non-parametric function to identify these individual mixture, effect modifier, and interacting variables, it is necessary to establish a pre-specified target parameter that is applied to variable sets when identified. Of course, both procedures cannot be done on the full data without resulting in
over-fitting the target parameter, therefore it is necessary to use cross-validation procedures where given a training set of the data is used to:

* Identify variable sets of interest using flexible machine learning
* Using these variables fit estimators for the relevant nuisance parameters for 
our final target parameter of interest
* Get estimates of our nuisance parameters and final target parameter of interest using the held-out validation data

This procedure is done in each fold of the CV procedure. Therefore, an estimate for each fold specific validation data is given. 

In order to optimize the optimum bias-variance trade-off for our causal parameter of interest we use targeted minimum loss based estimation (TMLE). 

### Method Overview

This vignette provides explanation for the package `SuperNOVA` which first builds a discrete Super Learner with a library of algorithms that are **basis function selectors** Using the best estimator from this algorithm (algorithm with the lowers cross-validated MSE) do **ANOVA style analysis** on the appropriate set of basis functions to determine which variable(s) contribute most to model fit through the basis functions. Depending on which sets of variables are data-adaptively determined, apply a target parameter of interest to these variable sets. The target parameters are explained in more details below.

First, let's load the packages we'll use and set a seed for simulation:

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(sl3)
library(SuperLearner)
library(devtools)
library(kableExtra)
load_all()
library(SuperNOVA)

set.seed(429151)
```

## Data and Notation

### Our Data-Generating System 

Consider $n$ observed units $O_1, \ldots, O_n$, where each random variable $O =(V \subset W, A, Y)$ corresponds to a single observational unit. Let $W$ denote baseline covariates (e.g., age, sex, education level) that are not considered by the 
analyst to be effect modifiers of any exposure variable, $V$ denote a subset of baseline covariates (measured before exposure) that may be potential effect modifiers of any exposure, $A$ denote a single exposure variable or a vector of
exposure variables (mixture) of interest (e.g., mixed pesticide biomarkers, metals etc. that are measured at the same time), and $Y$ an outcome of interest (e.g.,disease status). Though it need not be the case, let $A$ be continuous-valued, i.e. $A \in \mathbb{R}$. Let $O_i \sim \mathcal{P} \in \mathcal{M}$, where $\mathcal{M}$ is the nonparametric statistical model defined as the set of continuous densities on $O$ with respect to some dominating measure. To formalize the definition of stochastic interventions and their corresponding causal effects, we introduce a nonparametric structural equation model (NPSEM), based on @pearl2000causality, to define how the system changes under posited interventions on:

\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}

Where the set of structural equations provide a mechanistic model by which the
observed data $O$ is assumed to have been generated. 

### Assumptions

There are several standard
assumptions embedded in the NPSEM -- specifically, a temporal ordering that
supposes that $Y$ occurs after $A$, which occurs after $W$; each variable (i.e.,
$\{W, A, Y\}$) is assumed to have been generated from its corresponding
deterministic function (i.e., $\{f_W, f_A, f_Y\}$) of the observed variables
that precede it temporally, as well as an exogenous variable, denoted by $U$;
lastly, each exogenous variable is assumed to contain all unobserved causes of
the corresponding observed variable.

### Factorizing the Likelihood

The likelihood of the data $O$ admits a factorization, wherein, for $p_0^O$,
the density of $O$ with respect to the product measure, the density evaluated
on a particular observation $o$ may be a written
\begin{equation}
  p_0^O(o) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w)
\end{equation}
where $q_{0, Y}$ is the conditional density of $Y$ given $(A, W)$ with respect
to some dominating measure, $q_{0, A}$ is the conditional density of $A$ given
$W$ with respect to dominating measure $\mu$, and $q_{0, W}$ is the density of
$W$ with respect to dominating measure $\nu$. Further, for ease of notation, let
$Q(A, W) = \mathbb{E}[Y \mid A, W]$, $g(A \mid W) = \mathbb{P}(A \mid W)$, and $q_W$ the
marginal distribution of $W$. These components of the likelihood will be
essential in developing an understanding of the manner in which stochastic
treatment regimes perturb a system and how a corresponding causal effect may be
evaluated. 

### Data-Adaptive Target Parameter: Effect Modification


For example, in `SuperNOVA` **time ordering is used to determine our causal parameters of interest**. 
That is, the analyst passes a vector $V$ of baseline covariates $W$ to `SuperNOVA` that may be effect modifiers of any mixture variable in the vector $A$. Here, the target causal parameter is effect modification of the shift parameter because $V$ is measured before $A$ in our NPSEM. The effect modification parameter is a measure of how different the counterfactual mean of $Y$ is given a shift in $A$ stratified on $V$. 

### Data-Adaptive Target Parameter: Interaction

Conversely, if sets of variables are found to be important in the data-adaptive procedure that are only in $A$ (a set of mixture components) because these variables are (in most cases) measured together at one time (such as biomarkers) the shift parameter is instead the counterfactual mean of $Y$ given a simultaneous shift in both variables in $A$. This is a measure of interaction, rather than effect modification, using our shift parameter because in effect modification, we intervene on only one $A$ across the natural strata of $V$ (unperturbed). Whereas in interaction shifts, we intervene on many $A$ variables. In the case that an individual variable in $V$ or $A$ is found to have a marginal impact on $Y$, then a simple univariate stochastic shift is done on this variable alone. 

### Overview Stochastic Shifts

Briefly, let $A$ denote a continuous-valued exposure or set of exposures, we assume that the distribution of $A$ conditional on $W = w$ has support in the interval $(l(w), u(w))$ -- for convenience, let this support be _a.e._ That is, the minimum natural value of treatment $A$ for an individual with covariates $W = w$ is $l(w)$; similarly, the maximum is $u(w)$. Then, a simple stochastic intervention, based on a shift $\delta$, may be defined

\begin{equation}\label{eqn:shift}
  d(a, w) =
  \begin{cases}
    a - \delta & \text{if } a > l(w) + \delta \\
    a & \text{if } a \leq l(w) + \delta,
  \end{cases}
\end{equation}

where $0 \leq \delta \leq u(w)$ is an arbitrary pre-specified value that defines the degree to which the observed value $A$ is to be shifted, where possible. In `SuperNOVA` there are **four different shift parameters** depending on what is found in the data-adaptive ANOVA within the cross-validated fold - these are discussed below. 

## Target Parameter 

Our target parameter of interest is the stochastic intervention of a data-adaptively identified exposure variable or variables. Stochastic interventions are a relatively simple yet flexible framework for defining realistic causal effects.

In contrast to intervention regimens such as the average treatment effect (ATE), stochastic interventions can be applied to nearly any type of exposure variable (binary, ordinal, continuous). The resulting stochastic shift parameter can be interpreted in a similar fashion as coefficients in a regression model.

### Types of Shifts

This `SuperNOVA` package examines the effects attributable to:

1. shifting the observed value of an **individual exposure** up or down 
by some scalar $\delta$.

2. shifting the observed value of an **individual exposure within strata** of an effect modifier up or down by some scalar $\delta$.

3. shifting the observed value of a **joint exposure** up or down by 
some scalar $\delta$.

4. shifting the observed value of a **joint exposure up or down within strata** by 
some scalar $\delta$  of an effect modifier. 


The `SuperNOVA` package implements algorithms to data-adaptively identify mixture variables $A$ and effect modifiers $V$ that are associated  with an outcome and for these variable sets computes targeted minimum loss-based (TML) estimates of the counterfactual mean changes induced by a shifting function $\delta(A,W)$. 

## Data-Adative Variable Set Selection
1. SuperNOVA first fits an unrestricted Super Learner for $E(Y|W) = h_1(W)$, where $W$ is a set of baseline covariates that the analyst wants to control for, that is, baseline covariates with no suspected effect modification with exposures $A$. 

2. Residuals are calculated, $Y^*$, and a second Super Learner is fit $E(Y^*|A,V) = h_2(A,V)$ Here, for $h_2$ we only consider models for the conditional mean of $Y^*$ given $A,V$ that are a function of linear spline terms and their tensor products. 

3. Given a fit of such a model, we can then partition the variance using classic ANOVA decompositions by testing the null that some set of coefficient values (the $\beta_{s,i}$) are 0 (e.g., all terms that contain a particular variable, or represent two-way basis functions). 
4. We can then aggregate partial F-statistics for the basis functions up to the variable level for variable/interaction importance measures. Algorithms used in `SuperNOVA` return tensor products of arbitrary order and include `earth`, `polySpline` and `hal9001`. 

### Basis functions

Indicator variables can be used to indicate if a variable $A$ is less than or equal to a specific value $a_s$. The same can be done for $A_1, A_2$ to determine if both variable are less than or equal to a specific value. Thus, a function of our outcome is written as: 

$$\psi_{\beta} = \beta_0 + \sum_{s\subset \{1,2,...,p\}}\sum_{i=1}^{n} \beta_{s,i} \phi_{s,i},
    \text{ where } \phi_{s,i} = I(\tilde{A}_{i,s} \leq w_s), A \in \mathbb{R}^p$$ and $s$ denotes indices of subsets of the $A$ (e.g., both functions of single variables and two variables).The placement of knot-points for every potential value is not feasible in most real-world scenarios and therefore `SuperNOVA` chooses the best estimator from a class of basis algorithms. This approximates the exhaustive function.
    
### Defining Important Basis Functions

The user can pass into `SuperNOVA` the parameter `quantile_thresh` which designates the F-statistic threshold by which basis functions are kept. For example, if the user uses `quantile_thresh = 0.25` and the resulting vector is $M_1, M_1V_1, M_1M_3$ then this indicates that the basis functions that had an F-statistic above the 25th quantile included basis functions for $M_1$ alone, basis functions for $M_1$ and $V_1$ and basis functions for $M_1$ and $M_3$. 

Because this is done in V-fold cross-validation, this means that, within this fold we would train estimators for each of our estimates of interest, that is, the individual stochastic shift of $M_1$, how $V_1$ modifies the shift relationship between $M_1$ and $Y$, and the joint shift of $M1,M_3$. Now let's more formally define these parameters. 

## The Interaction Target Parameter

Once `SuperNOVA` has identified interacting variables with the highest partial F-statistic from the basis functions we need to get statistical estimation for how these interactions affect our outcome of interest.

Our target parameter of interest is a semi-parametric definition of an interaction, defined as: \begin{equation} \label{eq1}
\begin{split}
E(Y|A_{i^*} &+ \delta_1, A_{j^*} + \delta_2, W) - \\ 
E(Y|A_{i^*} &+ \delta_1, A_{j^*}, W) - \\ 
E(Y|A_{j^*} &+ \delta_2,  A_{i^*}, W) + \\
E(Y|A_{i^*}&, A_{j^*}, W) 
\end{split}
\end{equation}

For the bivariate case A $\in \{A_{i^*}, A_{j^*}$\} - where $A^*$ indicates a variable identified data-adaptively in the fold specific ANOVA procedure.

This joint causal quantity of interest is identified by: $E[Y_{g^o_{\delta_1, \delta_2}}] = E_W [\int_a E(Y|A=a, W)g^o(a|W)]$, $g^o_{\delta_1,\delta_2}(a|W)$ is similar to the propensity score estimator but here we are estimating joint densities.

Where in the bivariate case: 
$$g^o_{\delta_1, \delta_2}(a_1, a_2|w) = g(a_1 + \delta_1, a_2 + \delta_2|W = w)$$

Which can be estimated as: 

$$g^o_{\delta_1, \delta_2}(a_1, a_2|w) =  g(a_1 + \delta_1|W = w) * g(a_2 + \delta_2|W = w, A = a_1)$$. 

We can write our interaction target parameter as: 

$$E[Y_{g^o_{a\delta_1,\delta_2}}] - E[Y_{g^o_{\delta_1}}] - E[Y_{g^o_{\delta_2}}] + E[Y_{g^o_{1,2}}]$$ 

Where: 

\begin{equation} \label{eq2}
\begin{split}
E[Y_{g^o_{\delta_1,\delta_2}}] &= E_W [\int_{a_{1} a_{2}} E(Y|A=\{a_{1,\delta_1},a_{2,\delta_2}\} , W)g^o_{\delta_1,\delta_2}(a|W)]\\
E[Y_{g^o_{\delta_1}}] &= E_W [\int_{a_1} E(Y|A=a_{1, \delta_1}, W)g^o_{\delta_1}(a|W)]\\
E[Y_{g^o_{\delta_2}}] &= E_W [\int_{a_2} E(Y|A=a_{2, \delta_2}, W)g^o_{\delta_2}(a|W)]\\
E[Y_{g^o_{1,2}}] &= E_W [\int_{a_1,a_2} E(Y|A=a_1,a_2, W)g^o_{1,2}(a|W)]
\end{split}
\end{equation}

We can think of this interaction target parameter as the expected outcome given a joint shift in both exposure variables marginalized over the joint densities of both variables given both delta shifts subtracted by the expected outcome given both individual shifts marginalized over each shifted density respectively and adding back in the expected outcome under no shift marginalized over the joint density under no shift intervention. 


## Target Parameter: Effect Modification

Our effect modification follows in the same vein but instead looks at the difference in expected outcome under a shift intervention in an exposure between strata of an effect modifying variable. In the bivariate case where $V$ is a binary effect modifier this looks like: 

$$E[Y_{g^o_{\delta}} - Y | V = 1] - E[Y_{g^o_{\delta}} - Y | V = 0]$$

Where: 

\begin{equation} \label{eq3}
\begin{split}
E[Y_{g^o_{a_\delta}}] &= E_W [\int_{a_{\delta}} E(Y|A=a_{\delta}, W)g^o_{\delta}(a | V = v, W)]
\end{split}
\end{equation}

Effectively this is the difference of differences. This can be interpreted as the expected outcome difference given a shift in $A$ within strata $V = 1$ and $V = 0$ compared to the difference in expected mean outcomes under no shift when $V = 1$ and $V = 0$.


## Target Parameter: Individual Stochastic Shift

In the case where a basis function consists of only 1 variable that is included in $A$ or $V$ and individual stochastic shift is done and compared to the expected mean under no shift, which is simply: 

$$E[Y_{g^o_{\delta}}] - E[Y]$$
Where: 

$$E[Y_{g^o_{\delta}}] = E_W [\int_{a} E(Y|A=a_{ \delta}, W)g^o_{\delta}(a|W)]$$
$$E[Y] = E_W [\int_{a} E(Y|A=a, W)g(a|W)]$$


Or simply, the expected outcome given a shift in $A$ by $\delta$ marginalized over the conditional distribution of $a$ given such a shift minus the conditional expectation of $Y$ under no shift intervention.

## Estimating the Causal Effect for each Stochastic Intervention 

With identification assumptions satisfied, the efficient influence function (EIF) with respect to
the nonparametric model $\mathcal{M}$ can be estimated for each of our parameters of interested based on work from @diaz2012population and @diaz2018stochastic. The EIF for a stochastic intervention is: 

\begin{equation}
  D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
  \overline{Q}(d(a, w), w) - \Psi(P_0),
 
\end{equation}
where the auxiliary covariate $H(a,w)$ may be expressed
\begin{equation}
  H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)}
  {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w)),
  
\end{equation}
which may be reduced to
\begin{equation}
  H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1
 
\end{equation}
in the case that the treatment is within the limits that arise from conditioning
on $W$, i.e., for $A_i \in (u(w) - \delta, u(w))$.

This efficient influence function was originally derived to allow for the construction of a
semiparametric-efficient estimators for a stochastic shift in the univariate setting. The argeted maximum likelihood (TML) estimator to solve this EIF was formulated in @diaz2018stochastic by using the following steps: 

1. Construct initial density estimators $g_n$ of $g_0(A, W)$ and $Q_n$ of
   $\overline{Q}_0(A, W)$, using data-adaptive regression techniques.
2. Predict the density distribution of $d(A,W)$ under the shift $\delta$
3. For each observation $i$, compute an estimate $H_n(a_i, w_i)$ of the
   auxiliary covariate $H(a_i,w_i)$. This is the ratio of conditional densities, the shifted density in the numerator and natural density in the denominator.
4. Estimate the parameter $\epsilon$ in the logistic regression model
   $$ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
   \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),$$
   or an alternative regression model incorporating weights.
5. Compute TML estimator $\Psi_n$ of the target parameter, defining update
   $\overline{Q}_n^{\star}$ of the initial estimate
   $\overline{Q}_{n, \epsilon_n}$:
   \begin{equation}
     \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
     \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
   \end{equation}
   
### EIF and TMLE Applied to our Target Parameters


The same EIF and TMLE steps to solve the EIF used in the univariate stochastic shift of a treatment can be used for all our target parameters of interest. That is, in the case that only one variable in $A,V$ was found data-adaptively in basis functions, then the targeting step is exactly the same as in the original stochastic intervention work. In the case of effect modification, the same procedure is done however the conditional densities used in construction of the clever covariate include the counterfactuals made on the effect modifier. Construction of initial density estimators for $g_n$ of $g_0(A, V=v, W)$ and $Q_n$ of $\overline{Q}_0(A, V=v, W)$, using data-adaptive regression techniques. As can be seen, once these densities are estimated, the same TMLE steps can be done to update the original estimates. Likewise, for our interaction parameter the same procedure holds but now for joint densitieis. We construct initial density estimators $g_n$ of $g_0(A_1, A_2, W)$ and $Q_n$ of
$\overline{Q}_0(A_1, A_2, W)$, using data-adaptive regression techniques. Then our clever covariate becomes a ratio of the predicted joint likelihood of a joint shift over the unperturbed joint likelihood. The subsequent steps then all remain the same. 
  
## Data-Adaptive Esimation

The above theory works for stochastic shift situations where the $A$ is known \textit{a priori}. This is not the case for mixtures where we are interested in first identifying which variables are "important" given the fit of the best estimator and estimating the stochastic shift for these variable sets. Because the data is being used to both identify a target parameter and make estimates given this parameter, sample splitting (cross-validation) is used to avoid bias from over fitting, but still uses the entire data set to estimate a data-adaptive parameter. V-fold cross-validation involves: (i) ${1,..., n}$, observations, is divided into $V$ equal size subgroups, (ii) for each $v$, an estimation-sample, notationally $P_{n,v}$ , is defined by the v-th subgroup of size n/V, while the parameter-generating sample, $P_{n,v^c}$, is its complement. More concretely, for split $v$ the empirical distribution, $P_{n,v^c}$, is used to define the exposure(s) $A$ given our $\textit{a priori}$ basis function algorithm which outputs the target variables based on our ANOVA test of the basis functions used. The observations not in $P_{n,v^c}$, namely the empirical distribution for $P_{n,v}$ then is used to generate the parameter of interest. That is, if we had 1000 observations and 4 folds, our estimation sample $P_{n,v}$ would be of size 250. For each fold, the respective 750 observations would be used to train our estimators $g_n$ and $Q_n$ and given these estimators we then get predictions given the respective 250 observations. The predicted density, $g_n$ and outcome $Q_n$ are then used to construct our clever covariate for the shift intervention, $H_n(a_i, w_i)$ for the TMLE update within the fold. Therefore, TMLE updated estimates are given for each fold using the respective estimation sample data $P_{n,v}$. 

## Application:

Below we show implementation of `SuperNOVA` using simulated data. Because estimates are given for each data-adaptively identified parameter for each fold, after the demonstration we also show our pooled estimate approach for parameters that are found across all the folds. 

Simulate some toy data that has both interaction and effect modification.

```{r simulate data, eval = TRUE}
data_info <- SuperNOVA::simulate_data()
data <- data_info$data
summary(data)
```

```{r looking at simulated effect mod, fig.show="hold", eval = TRUE, warning = FALSE}
data_info$`plot 1`
data_info$`plot 3`
```

Partition the data into nodes that will be passed to `SuperNOVA`
```{r partition data, eval = TRUE}

W <- data[,c("W2", "W3")]
A <- data[,c("M1", "M2", "M3")]
V <- data[,c("W1")]
Y <- data[,"Y"]

```

Setup the Super Learner libraries that will be used for each of our nuisance parameters

```{r setup our Super Learner libraries, eval = TRUE}

# this library is used for E(Y|W) to calculate Y*, or the remaining variance in Y after removing variance due to W. 
SL.library <- c('SL.randomForest',
               "SL.glm",
               "SL.mean")

# this is our density estimator for g_n
sl_density_lrnr <- make_density_superlearner()


# this is our estimator for Q1_n - the Super Learner made of basis function estimators, E(Y*|A,V)


full_lrn_ridge <- Lrnr_glmnet$new(alpha = 0)
full_lrn_lasso <- Lrnr_glmnet$new(alpha = 1)
full_lrn_earth_1 <- Lrnr_earth$new(linpreds = FALSE, degree = 1, allowed = restrict_intxn_earth)
full_lrn_earth_2 <- Lrnr_earth$new(linpreds = FALSE, degree = 2, allowed = restrict_intxn_earth)
full_lrn_earth_3 <- Lrnr_earth$new(linpreds = FALSE, degree = 3, allowed = restrict_intxn_earth)
full_lrn_earth_4 <- Lrnr_earth$new(linpreds = FALSE, degree = 4, allowed = restrict_intxn_earth)

no_intxn_polymars <- restrict_intxn_polymars(data, 
                                             W_names =  colnames(W), 
                                             exposure_names = c(colnames(A), colnames(V)))

full_lrn_poly_3 <- Lrnr_polspline$new(knots = 3)
full_lrn_poly_4 <- Lrnr_polspline$new(knots = 4)
full_lrn_poly_5 <- Lrnr_polspline$new(knots = 5)
full_lrn_poly_6 <- Lrnr_polspline$new(knots = 6, no.interact = no_intxn_polymars)


full_lrn_glm <- Lrnr_glm$new()
full_lrn_mean <- Lrnr_mean$new()

Q1_learners <- c(
  # full_lrn_earth_1,
  # full_lrn_earth_2,
  # full_lrn_earth_3,
  # full_lrn_earth_4
  full_lrn_poly_3,
  full_lrn_poly_4,
  full_lrn_poly_5,
  full_lrn_poly_6

)

names(Q1_learners) <- c(
  # "full earth 1",
  # "full earth 2",
  # "full earth 3",
  # "full earth 4"
  "full poly 3",
  "full poly 4",
  "full poly 5",
  "full poly 6"
)

Q1_stack <- make_learner(Stack, Q1_learners)

# this is our estimator for Q2_n, the outcome estimation E(Y|A,W)
mean_lrnr <- Lrnr_mean$new()
fglm_lrnr <- Lrnr_glm_fast$new()
rf_lrnr <- Lrnr_ranger$new()
lasso_learner <- Lrnr_glmnet$new(alpha = 1)
ridge_learner <- Lrnr_glmnet$new(alpha = 0)
lrn_polspline <- Lrnr_polspline$new()
lrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)
hal_lrnr <- Lrnr_hal9001$new(max_degree = 3, n_folds = 3)

Q2_stack <- make_learner(
  Stack, mean_lrnr, fglm_lrnr, rf_lrnr, lasso_learner, ridge_learner, lrn_polspline, lrn_ranger100
)
```


Run the `SuperNOVA` main function passing in the data, number of folds, limit of detection censoring, delta to shift by, type of estimator and fluctuation, stacks of SL estimators for each nuisance parameter, quantile to threshold basis functions and whether to parallel process. 

```{r run SuperNOVA, eval = TRUE}
sim_results <- SuperNOVA::SuperNOVA(W = W,
                         V = V,
                         A = A,
                         Y = Y,
                         n_folds = 4,
                         LOD_cens = rep(1, dim(A)[1]),
                         delta = 1,
                         estimator = "tmle",
                         fluctuation = "standard",
                         max_iter = 10,
                         LOD_fit_args = list(
                         fit_type = c("glm"),
                         sl_learners = NULL
                         ),
                         sl_density_lrnr = sl_density_lrnr,
                         SL.library = SL.library,
                         Q1_stack = Q1_stack,
                         Q1_learners= Q1_learners,
                         Q2_stack = Q2_stack,
                         family = "gaussian",
                         quantile_thresh = 0.25,
                         parallel = TRUE) 

indiv_shift_results <- sim_results$`Indiv Shift Results`
em_results <- sim_results$`Effect Mod Results`
joint_shift_results <- sim_results$`Joint Shift Results`
```

## Pooling Estimates Found Across the Folds

Because `SuperNOVA` gets the TMLE updated shift parameter for each condition (individual, effect modification or joint), for each fold, when many folds are used the resulting tables can be very large. For estimates that are found across all folds (are consistently found in the data adaptive procedure) we can pool the effects across the folds similar to a meta-analysis. We should therefore give effect sizes with a higher precision (i.e. a smaller standard error derived from the efficient influence function (EIF)) a greater weight. If we want to calculate the pooled effect size under the fixed-effect model, we therefore simply use a weighted average of all studies. To calculate the weight $w_k$ for each fold $k$, we can use the standard error, which we square to obtain the variance $s^2_k$ of each effect size. Since a lower variance indicates higher precision, the inverse of the variance is used to determine the weight of each fold estimate.

$$w_k = \frac{1}{s^2_k}$$

Once we know the weights, we can calculate the weighted average, our estimate of the true pooled effect $\hat{\psi}$ We only have to multiply each foldâ€™s effect size $\hat{\theta}_k$
with its corresponding weight $w_k$, sum the results across all studies $K$ in our pooled analysis, and then divide by the sum of all the individual weights.

$$\hat{\psi} = \frac{\sum_{k = 1}^K \hat{\Psi}_k w_k}{\sum_{k = 1}^K w_k}$$
Because we use the inverse of the variance, we can call this parameter the inverse-variance pooled-fold-analysis.

The variance of the combined effect across the folds is defined as the reciprocal of the sum of the weights, or $v. = \frac{1}{\sum_{i=1}^K w_i}$ and likewise the standard error is $\sqrt{v.}$. Pooled confidence intervals and p-values are calculated in the normal way based on the calculated $\hat{\psi}$ and $v.$. Below we show the results tables output by the `SuperNOVA` for each parameter for each fold then we show output from the `compute_meta_results` function which calculates the pooled esimtates and creates plots. 

## Results Individual Stochastic Shifts

The expected outcome for individual stochastic shifting of data-adaptively identified exposures in a mixture is given in
`Indiv Shift Results` from the `SuperNOVA` fit object. 

```{r results indiv shift, eval = TRUE}
indiv_shift_results <- sim_results$`Indiv Shift Results`
setorder(indiv_shift_results, Condition)
indiv_shift_results

indiv_shift_results %>%
  kbl(caption = "Individual Stochastic Shift Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

The expected outcome for individual stochastic shifting of data-adaptively identified exposure in a mixture through strata of a data-adaptively identified effect modifier is given in`Effect Mod Results` from the `SuperNOVA` fit object. 

```{r results em shift, eval = TRUE}
em_results <- sim_results$`Effect Mod Results`
setorder(em_results, Fold)
em_results %>%
  kbl(caption = "Effect Modification Stochastic Shift Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
The expected outcome for joint stochastic shifting of data-adaptively identified set of exposures in a mixture is given in`Effect Mod Results` from the `SuperNOVA` fit object. 

```{r results joint shift, eval = TRUE}
joint_shift_results <- sim_results$`Joint Shift Results`
setorder(joint_shift_results, Fold)

joint_shift_results %>%
  kbl(caption = "Joint Stochastic Shift Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r plot indiv results, fig.show="hold", eval = TRUE}
indiv_plots <- compute_meta_results(indiv_shift_results, parameter = "Indiv Results")
indiv_plots$M1
indiv_plots$V
```

```{r plot effect mod results,  fig.show="hold", eval = TRUE}
em_plots <- compute_meta_results(em_results, parameter = "Effect Mod")
names(em_plots)
em_plots$`Effect Mod`
em_plots$`M1 V0`
em_plots$`M1 V1`
```

```{r plot joint shift results, fig.show="hold", eval = TRUE}
joint_plots <- compute_meta_results(joint_shift_results, parameter = "Joint Shift")
names(joint_plots)
joint_plots$`Psi-M1M2`
joint_plots$`Psi-M1M3`
```

